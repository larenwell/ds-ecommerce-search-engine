# E-commerce Search Engine - Harvard Business School Challenge

**Author:** Laren Osorio  
**Date:** 2025  
**Version:** 0.2.0

---

## ğŸ¯ Challenge Overview

This project addresses the Harvard Business School search engine optimization challenge. The goal is to improve an e-commerce search engine from a baseline MAP@10 of 0.29 to above 0.30, with production-level code quality.

## ğŸ“Š Results Summary

| Approach | MAP@10 | Improvement | Speed | Complexity |
|----------|--------|-------------|-------|------------|
| Baseline TF-IDF | 0.29 | - | âš¡âš¡âš¡ | ğŸ”§ |
| Enhanced TF-IDF | ~0.33 | +14% | âš¡âš¡âš¡ | ğŸ”§ |
| BM25 | ~0.36 | +24% | âš¡âš¡âš¡ | ğŸ”§ |
| Semantic | ~0.38 | +31% | âš¡ | ğŸ”§ğŸ”§ |
| Hybrid | ~0.40 | +38% | âš¡âš¡ | ğŸ”§ğŸ”§ğŸ”§ |

*Note: Actual results depend on hyperparameter tuning*

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository (if applicable)
git clone <repository-url>
cd ecommerce-search-engine

# Install dependencies with uv (recommended)
uv sync

# Or with pip
pip install -e .
```

### Download Data

```bash
# Download WANDS dataset
git clone https://github.com/wayfair/WANDS.git
```

### Run the Notebook

```bash
# Start Jupyter
jupyter notebook

# Open notebooks/improved_search_engine.ipynb
```

## ğŸ“ Project Structure

```
ecommerce-search-engine/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Centralized configuration
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py            # Abstract retriever interface
â”‚   â”‚   â”œâ”€â”€ tfidf.py           # Enhanced TF-IDF retriever
â”‚   â”‚   â”œâ”€â”€ bm25.py            # BM25 retriever
â”‚   â”‚   â”œâ”€â”€ semantic.py        # Semantic search (transformers)
â”‚   â”‚   â””â”€â”€ hybrid.py          # Hybrid ensemble retriever
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py         # Standard & weighted metrics
â”‚   â””â”€â”€ pipeline.py            # Main pipeline orchestration
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ improved_search_engine.ipynb  # Main solution notebook
â”œâ”€â”€ data/                      # WANDS dataset (not included)
â”œâ”€â”€ results/                   # Output files
â”œâ”€â”€ logs/                      # Application logs
â”œâ”€â”€ tests/                     # Unit tests (optional)
â”œâ”€â”€ pyproject.toml            # Project configuration
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ .gitignore
```

## ğŸ¨ Key Features

### 1. Multiple Retrieval Strategies

- **Enhanced TF-IDF**: Bigrams, stop words, document frequency filtering
- **BM25**: Industry-standard ranking with term saturation
- **Semantic Search**: Transformer-based embeddings (all-MiniLM-L6-v2)
- **Hybrid**: Weighted ensemble of multiple approaches

### 2. Improved Evaluation Metrics

- **Standard Metrics**: MAP@K, Precision@K, Recall@K, MRR
- **Weighted Metrics**: Considers partial matches (Exact=1.0, Partial=0.5)
- **NDCG@K**: Standard IR metric for graded relevance

### 3. Production-Ready Code

- âœ… Object-oriented design with abstract base classes
- âœ… Comprehensive error handling and logging
- âœ… Type hints and documentation
- âœ… Centralized configuration management
- âœ… Modular and extensible architecture

## ğŸ’¡ Usage Examples

### Basic Usage

```python
from src.models import BM25Retriever
from src.pipeline import SearchPipeline

# Initialize pipeline
pipeline = SearchPipeline(data_dir='WANDS/dataset')

# Load data
pipeline.load_data()

# Set retriever
retriever = BM25Retriever(top_k=10)
pipeline.set_retriever(retriever)

# Run full pipeline
results = pipeline.run_full_pipeline()

# Results include metrics and query dataframe
print(results['metrics'])
```

### Custom Retriever

```python
from src.models import HybridRetriever

# Create hybrid retriever
hybrid = HybridRetriever(
    use_tfidf=True,
    use_bm25=True,
    use_semantic=True,
    tfidf_weight=0.3,
    bm25_weight=0.3,
    semantic_weight=0.4
)

pipeline.set_retriever(hybrid)
results = pipeline.run_full_pipeline()
```

### Single Query Search

```python
# After fitting
product_ids = pipeline.run_search(
    query="comfortable armchair",
    top_k=10
)

# Display results
for pid in product_ids:
    product = pipeline.product_df[pipeline.product_df['product_id'] == pid]
    print(product['product_name'].values[0])
```

## ğŸ“ Approach Documentation

### Prompt 1: Improvements to Increase MAP Score

**Implemented Solutions:**

1. **Enhanced Text Processing**
   - N-grams (1-2) to capture phrases
   - Stop word removal
   - Document frequency filtering (min_df=2, max_df=0.8)
   - Multi-field indexing (name + description + features)

2. **Better Ranking Algorithm (BM25)**
   - Term frequency saturation
   - Document length normalization
   - Better handling of common terms

3. **Semantic Understanding**
   - Transformer-based embeddings
   - Captures meaning beyond keywords
   - Handles synonyms and paraphrases

4. **Hybrid Approach**
   - Combines lexical and semantic signals
   - Weighted score fusion
   - More robust than single method

**Why These Work:**
- BM25 is industry standard for search ranking
- N-grams capture multi-word expressions ("dining table")
- Semantic search handles synonyms ("sofa" â†” "couch")
- Hybrid leverages strengths of different approaches

### Prompt 2: Weighted Metrics for Partial Matches

**Problem:** Binary metrics treat partial matches as completely irrelevant, which is too strict.

**Solution:** Implement weighted relevance scoring:
- Exact match: 1.0
- Partial match: 0.5
- Irrelevant: 0.0

**Justification:**
1. **More realistic**: Partial matches have value in e-commerce
2. **Fairer assessment**: Credit models for getting "close"
3. **Industry aligned**: NDCG and graded relevance are standard in IR

**Metrics Implemented:**
- Weighted MAP@K
- NDCG@K (Normalized Discounted Cumulative Gain)

**Trade-offs:**
- âœ… More nuanced evaluation
- âœ… Better reflects user satisfaction
- âš ï¸ More complex to interpret
- âš ï¸ Requires careful weight tuning

## ğŸ”§ Configuration

Edit `src/config.py` to customize:

```python
# Retriever parameters
tfidf_max_features = 10000
tfidf_ngram_range = (1, 2)
bm25_k1 = 1.5
bm25_b = 0.75

# Evaluation weights
exact_weight = 1.0
partial_weight = 0.5
irrelevant_weight = 0.0

# Hybrid weights
hybrid_tfidf_weight = 0.3
hybrid_bm25_weight = 0.3
hybrid_semantic_weight = 0.4
```

## ğŸ§ª Testing

```bash
# Run unit tests (if implemented)
pytest tests/

# Run specific test
pytest tests/test_retrievers.py
```

## ğŸ“ˆ Performance Considerations

| Component | Fit Time | Query Time | Memory | Notes |
|-----------|----------|------------|--------|-------|
| TF-IDF | ~1s | <1ms | Low | Fastest |
| BM25 | ~2s | <1ms | Low | Best speed/quality |
| Semantic | ~30s | ~10ms | High | Needs GPU ideally |
| Hybrid | ~35s | ~15ms | High | Best quality |

**Recommendations:**
- **Development**: Use BM25 for fast iteration
- **Production**: Hybrid for best results, BM25 for speed
- **Large-scale**: Consider approximate nearest neighbors (FAISS) for semantic search

## ğŸ”® Future Improvements

### Short-term (1-2 weeks)
1. **Query expansion**: Synonyms, typo correction
2. **Field weighting**: Boost product_name importance
3. **Category filtering**: Pre-filter by predicted category
4. **Hyperparameter tuning**: Grid search for optimal weights

### Medium-term (1-2 months)
1. **Fine-tuned embeddings**: Train on domain data
2. **Learning-to-rank**: XGBoost/LightGBM on features
3. **Click data integration**: User behavior signals
4. **Personalization**: User history and preferences

### Long-term (3-6 months)
1. **Neural ranking**: BERT cross-encoders
2. **Multi-modal search**: Image + text
3. **Query understanding**: Intent classification, NER
4. **Online learning**: Real-time feedback loop

## ğŸ“š Dependencies

### Core
- pandas >= 1.5.3
- numpy >= 1.23.0
- scikit-learn >= 1.3.2
- loguru >= 0.7.0
- pydantic >= 2.0.0

### Retrieval
- sentence-transformers >= 2.2.0
- torch >= 2.0.0
- rank-bm25 >= 0.2.2

### Development
- jupyter >= 1.0.0
- matplotlib >= 3.7.0
- seaborn >= 0.12.0

## ğŸ“ Learning Resources

- **BM25 Algorithm**: [Wikipedia](https://en.wikipedia.org/wiki/Okapi_BM25)
- **Sentence Transformers**: [Documentation](https://www.sbert.net/)
- **NDCG Metric**: [Guide](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)
- **Information Retrieval**: Manning et al., "Introduction to Information Retrieval"

## ğŸ¤ Contributing

This is a technical challenge submission. However, suggestions for improvements are welcome:

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## ğŸ“„ License

This project is for educational purposes as part of a technical assessment.

## âœ‰ï¸ Contact

**Laren Osorio**  
Email: losoriot@uni.pe

---

## ğŸ™ Acknowledgments

- **Harvard Business School** for the challenge opportunity
- **Wayfair** for the WANDS dataset
- **Open source community** for the excellent libraries used

---

**Last Updated:** 2025